package com.aexp.gmdl.data.processor.service.impl;

import static com.aexp.gmdl.data.processor.constants.AppConstants.CORRELATIONID;
import static com.aexp.gmdl.data.processor.constants.AppConstants.KEY_ADDRESS;
import static com.aexp.gmdl.data.processor.utils.PreConditions.checkArgument;
import static com.aexp.gmdl.data.processor.utils.PreConditions.requireNotBlank;

import com.aexp.gmdl.data.processor.constants.AppConstants;
import com.aexp.gmdl.data.processor.constants.AppConstants.Status;
import com.aexp.gmdl.data.processor.service.MessageSubscriberService;
import com.aexp.gmdl.data.processor.utils.KafkaUtils;
import io.vertx.core.*;
import io.vertx.core.eventbus.Message;
import io.vertx.core.json.DecodeException;
import io.vertx.core.json.JsonArray;
import io.vertx.core.json.JsonObject;
import io.vertx.kafka.client.common.TopicPartition;
import io.vertx.kafka.client.consumer.KafkaConsumer;
import io.vertx.kafka.client.consumer.KafkaConsumerRecord;
import java.time.Duration;
import java.util.*;
import java.util.concurrent.ConcurrentHashMap;
import java.util.concurrent.atomic.AtomicLong;
import java.util.concurrent.atomic.AtomicReference;
import java.util.stream.Collectors;
import java.util.stream.IntStream;
import org.apache.kafka.clients.consumer.ConsumerConfig;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

public class MessageSubscriberServiceImpl implements MessageSubscriberService {

  private static final Logger LOGGER = LoggerFactory.getLogger(MessageSubscriberServiceImpl.class);
  public static final String REASON = "reason";
  private final Vertx vertx;
  private final Map<String, String> kafkaConsumerConfig;
  private final AtomicReference<Map<String, KafkaConsumer<String, String>>> kafkaConsumersList =
      new AtomicReference<>(new ConcurrentHashMap<>());
  private final AtomicReference<Map<String, Long>> tableCounter =
      new AtomicReference<>(new ConcurrentHashMap<>());

  public MessageSubscriberServiceImpl(Vertx vertx, Map<String, String> kafkaConsumerConfig) {
    Objects.requireNonNull(vertx, "vertx is required and shouldn't be null.");
    Objects.requireNonNull(
        kafkaConsumerConfig, "kafkaConsumerConfig is required and shouldn't be null.");

    this.vertx = vertx;
    this.kafkaConsumerConfig = kafkaConsumerConfig;
  }

  @Override
  public Future<Void> createMessageSubscriber(
      String topic, String destinationAddress, long pollTimeout, JsonObject config) {
    requireNotBlank(topic, "topic is required and missing.");
    requireNotBlank(destinationAddress, "destinationAddress is required and missing.");
    checkArgument((pollTimeout > 0), "pollTimeout should be > 0");

    Promise<Void> promise = Promise.promise();

    kafkaConsumerConfig.put(
        ConsumerConfig.MAX_POLL_RECORDS_CONFIG, config.getString("max.poll.records"));
    KafkaConsumer<String, String> kafkaConsumer = KafkaConsumer.create(vertx, kafkaConsumerConfig);
    kafkaConsumersList.get().put(destinationAddress, kafkaConsumer);
    tableCounter.get().put(destinationAddress, 0L);
    // registering handlers for assigned and revoked partitions
    kafkaConsumer.partitionsAssignedHandler(
        topicPartitions -> {
          for (TopicPartition topicPartition : topicPartitions) {
            LOGGER.info(
                "Partitions assigned - {} {}",
                topicPartition.getTopic(),
                topicPartition.getPartition());
          }
        });

    kafkaConsumer.partitionsRevokedHandler(
        topicPartitions -> {
          for (TopicPartition topicPartition : topicPartitions) {
            LOGGER.info(
                "Partitions revoked - {} {}",
                topicPartition.getTopic(),
                topicPartition.getPartition());
          }
        });
    kafkaConsumer
        .subscribe(topic)
        .onSuccess(
            ignore -> {
              LOGGER.debug("Successfully subscribed to kafka topic: {}", topic);
              poll(pollTimeout, destinationAddress);
              promise.complete();
            })
        .onFailure(
            throwable -> {
              LOGGER.error(
                  "Failed to subscribe to kafka topic: {}, Cause: {}",
                  topic,
                  throwable.getLocalizedMessage(),
                  throwable);
              promise.fail(throwable);
            });

    vertx.eventBus().<JsonObject>consumer(
            "kafka-commit-task",
            response -> {
              JsonObject objectBody = response.body();
              JsonArray array = objectBody.getJsonArray("data");
              String address = objectBody.getString(KEY_ADDRESS);
              String correlationId = objectBody.getString(CORRELATIONID);

              LOGGER.info(
                  "CorrelationId: {}, address: {}, kafka-commit-task {} to publish",
                  correlationId,
                  address,
                  array.size());

              if (array.size() > 0) {

                array.stream()
                    .forEach(
                        json -> {
                          JsonObject jsonObj = (JsonObject) json;
                          String corrIdRow = jsonObj.getString(CORRELATIONID);
                          String addressRow = jsonObj.getString(KEY_ADDRESS);
                          JsonObject commitData = jsonObj.getJsonObject("commitData");

                          if (commitData == null) {
                            LOGGER.error(
                                "BatchCorrelationId: {} ,address:{} addressRow :{}, wrong - addressRow or corrIdRow not found in commit map  address:{}",
                                correlationId,
                                address,
                                addressRow,
                                corrIdRow);
                          }
                        });

              } else {

                LOGGER.error("BatchCorrelationId: {}, Empty array {}", correlationId, array);
              }

              response.reply("Batch Completed");

              LOGGER.info(
                  "BatchCorrelationId: {} Batch completed for address:{},batch size:{}",
                  correlationId,
                  address,
                  array.size());
            });

    vertx
        .eventBus()
        .<JsonObject>consumer(
            "batch-complete",
            response -> {
              JsonObject objectBody = response.body();
              String address = objectBody.getString(KEY_ADDRESS);
              String correlationId = objectBody.getString(CORRELATIONID);
              LOGGER.info(
                  "CorrelationId: {}, batch-complete for the Address: {}", correlationId, address);

              tableCounter.get().put(address, 0L);
              response.reply("correlationId: " + correlationId + ", address resumed :" + address);
            });
    return promise.future();
  }

  @Override
  public Future<Void> closeConsumers() {
    List<Future> futures =
        kafkaConsumersList.get().values().stream()
            .filter(Objects::nonNull)
            .map(KafkaConsumer::close)
            .collect(Collectors.toList());

    return CompositeFuture.all(futures).mapEmpty();
  }

  private void poll(long pollTimeout, String address2) {

    AtomicLong startTime = new AtomicLong();
    Handler<Long> action2 =
        id -> {
          if (tableCounter.get().get(address2) > 0) {
            if (System.currentTimeMillis() - tableCounter.get().get(address2 + "-time") > 120000) {
              tableCounter.get().put(address2, 0L);
              LOGGER.info("2 minute job was sleeping for address {}, resetting to zero", address2);
            }

          } else {
            tableCounter.get().put(address2, 1L);
            tableCounter.get().put(address2 + "-time", System.currentTimeMillis());
            String batchCorrelationId = UUID.randomUUID().toString();

            try {

              kafkaConsumersList.get().get(address2)
                  .poll(Duration.ofMillis(pollTimeout)).onFailure(error ->
                          LOGGER.error(
                              "BatchCorrelationId: {}, Kafka polling error. Cause: {}, stacktrace: {}",
                              batchCorrelationId,
                              error.getMessage(),
                              error))
                  .compose(
                      records -> {
                        List<Future> futures;
                        futures = new ArrayList<>();
                        if (records.size() > 0) {
                          startTime.set(System.currentTimeMillis());
                          LOGGER.info(
                              "BatchCorrelationId: {}, Kafka polled msg {} records. address {} , first row sample data key: {}",
                              batchCorrelationId,
                              records.size(),
                              address2,
                              records.size() > 0 ? records.recordAt(0).key() : null);
                          DuplicateTracker duplicateTracker = new DuplicateTracker();
                          futures =
                              IntStream.range(0, records.size())
                                  .mapToObj(records::recordAt)
                                  .map(
                                      recordFromKafka ->
                                          processMessage(
                                              recordFromKafka,
                                              address2,
                                              batchCorrelationId,
                                              UUID.randomUUID().toString(),
                                              duplicateTracker))
                                  .collect(Collectors.toList());

                          LOGGER.info(
                              "BatchCorrelationId: {}, Database Count COUNT: {}, Non DB COUNT: {}",
                              batchCorrelationId,
                              duplicateTracker.getDbrecordCount(),
                              duplicateTracker.getNonDBrecordCount());

                        } else {
                          Promise<Void> norecordPromise = Promise.promise();
                          norecordPromise.fail("NO_RECORD");
                          futures.add(norecordPromise.future());
                        }
                        return CompositeFuture.join(futures);
                      })
                  .onComplete(
                      ar -> {
                        if (ar.succeeded()) {
                          AtomicReference<Boolean> dbPending = new AtomicReference<>(false);
                          ar.result().list().stream()
                              .forEach(
                                  data -> {
                                    if (data.equals("DB")) {
                                      dbPending.set(true);
                                    }
                                  });

                          if (Boolean.TRUE.equals(dbPending.get())) {
                            LOGGER.info(
                                "BatchCorrelationId: {} , address: {} , DB processing  pending",
                                batchCorrelationId,
                                address2);
                          } else {
                            LOGGER.debug("address:{} , DB processing not pending", address2);
                            tableCounter.get().put(address2, 0L);
                          }
                        } else {
                          AtomicReference<Boolean> dbPending = new AtomicReference<>(false);

                          if (!ar.cause().getLocalizedMessage().equals("NO_RECORD")) {
                            LOGGER.error(
                                "BatchCorrelationId: {}, Error in processing kafka messages. Cause: {}, stacktrace: {}",
                                batchCorrelationId,
                                ar.cause().getLocalizedMessage(),
                                ar.cause());
                          }
                          if (Boolean.TRUE.equals(dbPending.get())) {
                            LOGGER.info(
                                "BatchCorrelationId: {} address: {} , DB processing  pending",
                                batchCorrelationId,
                                address2);
                            LOGGER.info(
                                "BatchCorrelationId: {} address: {} , DB processing  pending",
                                batchCorrelationId,
                                address2);
                          } else {
                            LOGGER.debug("address:{} , DB processing not pending", address2);
                            tableCounter.get().put(address2, 0L);
                          }
                        }
                      });

            } catch (Exception e) {
              tableCounter.get().put(address2, 0L);
              LOGGER.error(
                  "BatchCorrelationId: {}, Kafka Unknown error: {}, stacktrace :{}",
                  batchCorrelationId,
                  e.getLocalizedMessage(),
                  e);
            }
          }
        };
    vertx.setPeriodic(200, action2);
  }

  private Future<String> processMessage(
      KafkaConsumerRecord<String, String> recordFromKafka,
      String address,
      String batchCorrelationId,
      String correlationId,
      DuplicateTracker duplicateTracker) {
    Promise<String> promise = Promise.promise();
    try {

      String key = recordFromKafka.key();
      String topic = recordFromKafka.topic();

      boolean isNotDuplicate = true;

      LOGGER.info(
          "Received msg from Kafka consumer topic: {}, address: {}, headers: {}, key: {}, partition: {}, offset: {}, BatchCorrelationId: {}, CorrelationId: {}, isNotDuplicate: {}",
          topic,
          address,
          recordFromKafka.headers(),
          key,
          recordFromKafka.partition(),
          recordFromKafka.offset(),
          batchCorrelationId,
          correlationId,
          isNotDuplicate);

      // Send the received message (data + headers) to destination Event Bus address.
      JsonObject eventBusRequest = new JsonObject();
      if (recordFromKafka.value() != null) {
        eventBusRequest.put(AppConstants.KEY_DATA, new JsonObject(recordFromKafka.value()));
      } else if (recordFromKafka.key() != null) {
        eventBusRequest.put(AppConstants.KEY_DATA, new JsonObject(recordFromKafka.key()));
      }

      eventBusRequest.put(
          AppConstants.KEY_HEADERS,
          new JsonObject()
              .put(
                  AppConstants.KEY_EVENT_TYPE,
                  KafkaUtils.getHeaderValue(recordFromKafka.headers(), "IBM-A_ENTTYP")));
      // Added Header param as DL in case of missing and recordFromKafka.key present
      if ((eventBusRequest
                  .getJsonObject(AppConstants.KEY_HEADERS)
                  .getString(AppConstants.KEY_EVENT_TYPE)
                  .equals("")
              || recordFromKafka.value() == null)
          && recordFromKafka.key() != null) {
        eventBusRequest
            .getJsonObject(AppConstants.KEY_HEADERS)
            .put(AppConstants.KEY_EVENT_TYPE, AppConstants.EVENT_DELETE);
      }
      eventBusRequest
          .getJsonObject(AppConstants.KEY_HEADERS)
          .put(AppConstants.CORRELATION_ID, correlationId);
      eventBusRequest
          .getJsonObject(AppConstants.KEY_HEADERS)
          .put(AppConstants.BATCH_CORRELATION_ID, batchCorrelationId);
      eventBusRequest.put(
          AppConstants.KAFKA_KEY_DATA,
          recordFromKafka.key() != null
              ? new JsonObject(recordFromKafka.key())
              : recordFromKafka.value());

      eventBusRequest
          .getJsonObject(AppConstants.KEY_HEADERS)
          .put(AppConstants.IS_NOT_DUPLICATE, isNotDuplicate);
      String finalCorrelationId = correlationId;

      JsonObject commitData =
          new JsonObject()
              .put("topic", recordFromKafka.topic())
              .put("offset", recordFromKafka.offset() + 1)
              .put("partition", recordFromKafka.partition());
      eventBusRequest.put("commitData", commitData);

      vertx
          .eventBus()
          .<JsonObject>request(
              address,
              eventBusRequest,
              replyMsg ->
                  handleResponse(
                      replyMsg,
                      promise,
                      recordFromKafka,
                      batchCorrelationId,
                      finalCorrelationId,
                      address,
                      duplicateTracker));

    } catch (DecodeException e) {
      LOGGER.error(
          "CorrelationId: {}, Decoding error:: bad message in topic. Cause: {}",
          correlationId,
          e.getMessage(),
          e);
      promise.fail(e);
    } catch (Exception e) {
      LOGGER.error(
          "CorrelationId: {} Generic exception in handling the kafka consumer message. Cause: {}",
          correlationId,
          e.getMessage(),
          e);
      promise.fail(e);
    }
    return promise.future();
  }

  private void handleResponse(
      AsyncResult<Message<JsonObject>> replyMsg,
      Promise<String> promise,
      KafkaConsumerRecord<String, String> recordFromKafka,
      String batchCorrelationId,
      String correlationId,
      String address,
      DuplicateTracker duplicateTracker) {

    if (replyMsg.succeeded()) {
      LOGGER.debug(
          "Sending response from MessageSubscriberServiceImpl.handleResponse.success {}, CorrelationId: {}",
          replyMsg.result().body(),
          correlationId);
      JsonObject reply = replyMsg.result().body();

      String status = reply.getString("status").toLowerCase();

      if (Status.from(status).equals(Status.SUCCESS)) {
        promise.complete("DB");
        duplicateTracker.addDB();
      } else {

        if (replyMsg.result().body().getString(AppConstants.FAILED_REASON) != null
            && replyMsg
                .result()
                .body()
                .getString(AppConstants.FAILED_REASON)
                .contains(AppConstants.FILTER_CRITERIA_NOT_MATCHING)) {

          promise.complete("NDB");
          duplicateTracker.addNonDB();
        } else {
          LOGGER.error(
              "BatchCorrelationId: {}, CorrelationId: {}, Kafka commit map (process- error) for address:{} : {} ",
              batchCorrelationId,
              correlationId,
              address,
              replyMsg);
          promise.complete("NDB");
          duplicateTracker.addNonDB();
        }
      }
    } else {

      if (replyMsg.cause().getLocalizedMessage() != null
          && replyMsg
              .cause()
              .getLocalizedMessage()
              .contains(AppConstants.FILTER_CRITERIA_NOT_MATCHING)) {

        String key = recordFromKafka.key();
        LOGGER.info(
            "BatchCorrelationId: {}, CorrelationId: {},FILTER_CRITERIA_NOT_MATCHING table : {}, key: {}, header: {}",
            batchCorrelationId,
            correlationId,
            address,
            key,
            recordFromKafka.headers());
        promise.complete("NDB");
        duplicateTracker.addNonDB();
      } else {
        String key = recordFromKafka.key();
        String value = recordFromKafka.value();
        LOGGER.error(
            "BatchCorrelationId: {}, CorrelationId: {},EventBus Reply Failure.address:{}, Cause: {},value: {},key: {},header: {}, correlationId: {},replyMsg:{}",
            batchCorrelationId,
            correlationId,
            address,
            replyMsg.cause().getMessage(),
            value,
            key,
            recordFromKafka.headers(),
            correlationId,
            replyMsg);
        promise.complete("NDB");
        duplicateTracker.addNonDB();
      }
    }
  }
}


private void handleProcessors(
      String address,
      JsonObject content,
      JsonObject processorConfigElements,
      String id,
      Promise<Void> promise,
      String timeZone,
      DelayBridge delayDeleteBridge) {
    List<Future> futures = new ArrayList<>();

    JsonObject data = content.getJsonObject(KEY_DATA);
    String tableName = processorConfigElements.getString("id").replace("-", "_");
    String correlationId = content.getJsonObject(KEY_HEADERS).getString(CORRELATION_ID);
    String batchCorrelationId = content.getJsonObject(KEY_HEADERS).getString(BATCH_CORRELATION_ID);

    String eventType =
        processorConfigElements.getString(KEY_EVENT_TYPE) == null
            ? ""
            : processorConfigElements.getString(KEY_EVENT_TYPE).trim();

   if (ProcessorUtil.isMatchingToFilterCriteria(
        data,
        processorConfigElements.getJsonObject(CONFIG_FILTERING_CRITERIA),
        processorConfigElements.getString("id"))) {

if (Boolean.TRUE.equals(
          processorConfigElements.getBoolean(CONFIG_ENABLE_POSTGRES_PERSISTENCE))) {
if (eventType.equalsIgnoreCase(EVENT_UPDATE)
              || eventType.equalsIgnoreCase(EVENT_INSERT)
              || eventType.equalsIgnoreCase(EVENT_UPDATE_RR)) {
            LOGGER.debug(
                "BatchCorrelationId: {}, CorrelationId: {}, Validation enabled for  table: {} and action: {}",
                batchCorrelationId,
                correlationId,
                tableName,
                eventType);
            Promise<Void> dbPromise = Promise.promise();
            futures.add(dbPromise.future());
            Future<JsonArray> validationPromise =
                validateDataWithTargetDB(readCache, tableName, data);

            validationPromise.onComplete(
                valResult -> {
                  if (valResult.succeeded()) {
                    if (valResult.result().isEmpty()) {

                      JsonArray upsertParam = null;
                      switch (id) {
                        case EventBusConstants.CDC_MER_PRIM_CHAR_DTL_EB:
                          upsertParam = buildMerPrimCharDtlParams(data, timeZone);
